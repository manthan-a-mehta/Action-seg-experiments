---------- Experiment Configuration ----------
{'batch_size': 1,
 'boundary_th': 0.5,
 'ce': True,
 'ce_weight': 1.0,
 'class_weight': True,
 'csv_dir': './csv',
 'dampening': 0.0,
 'dataset': '50salads',
 'dataset_dir': './dataset',
 'focal': False,
 'focal_weight': 1.0,
 'gstmse': True,
 'gstmse_index': 'feature',
 'gstmse_weight': 1.0,
 'in_channel': 2048,
 'lambda_b': 0.1,
 'learning_rate': 0.0005,
 'max_epoch': 50,
 'model': 'ActionSegmentRefinementNetwork',
 'momentum': 0.9,
 'n_features': 64,
 'n_layers': 10,
 'n_stages': 4,
 'n_stages_asb': 4,
 'n_stages_brb': 4,
 'nesterov': True,
 'num_workers': 4,
 'optimizer': 'Adam',
 'param_search': False,
 'split': 1,
 'tmse': False,
 'tmse_weight': 0.15,
 'tolerance': 5,
 'use_focal': True,
 'weight_decay': 0.0001}
---------- Loading Model ----------
Adam will be used as an optimizer.
focal loss used: 
---------- Start training ----------
0
/home/fish/Balaji/Documents/code/NEU-Fall2021/image_captioning/pyenv_1.13/lib/python3.6/site-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.
  warnings.warn(warning.format(ret))
epoch: 0	lr: 0.0005	train loss: 0.7932
1
epoch: 1	lr: 0.0005	train loss: 0.6199
2
epoch: 2	lr: 0.0005	train loss: 0.4519
3
epoch: 3	lr: 0.0005	train loss: 0.3587
4
epoch: 4	lr: 0.0005	train loss: 0.2854
5
epoch: 5	lr: 0.0005	train loss: 0.3095
6
epoch: 6	lr: 0.0005	train loss: 0.2098
7
epoch: 7	lr: 0.0005	train loss: 0.2031
8
epoch: 8	lr: 0.0005	train loss: 0.1702
9
epoch: 9	lr: 0.0005	train loss: 0.1345
10
epoch: 10	lr: 0.0005	train loss: 0.1344
11
epoch: 11	lr: 0.0005	train loss: 0.1340
12
epoch: 12	lr: 0.0005	train loss: 0.0996
13
epoch: 13	lr: 0.0005	train loss: 0.0914
14
epoch: 14	lr: 0.0005	train loss: 0.0868
15
epoch: 15	lr: 0.0005	train loss: 0.0785
16
epoch: 16	lr: 0.0005	train loss: 0.0788
17
epoch: 17	lr: 0.0005	train loss: 0.1035
18
epoch: 18	lr: 0.0005	train loss: 0.0767
19
epoch: 19	lr: 0.0005	train loss: 0.0636
20
epoch: 20	lr: 0.0005	train loss: 0.0564
21
epoch: 21	lr: 0.0005	train loss: 0.0584
22
epoch: 22	lr: 0.0005	train loss: 0.0620
23
epoch: 23	lr: 0.0005	train loss: 0.0551
24
epoch: 24	lr: 0.0005	train loss: 0.0521
25
epoch: 25	lr: 0.0005	train loss: 0.0523
26
epoch: 26	lr: 0.0005	train loss: 0.0512
27
epoch: 27	lr: 0.0005	train loss: 0.0502
28
epoch: 28	lr: 0.0005	train loss: 0.0468
29
epoch: 29	lr: 0.0005	train loss: 0.0483
30
epoch: 30	lr: 0.0005	train loss: 0.0522
31
epoch: 31	lr: 0.0005	train loss: 0.1107
32
epoch: 32	lr: 0.0005	train loss: 0.1043
33
epoch: 33	lr: 0.0005	train loss: 0.0592
34
epoch: 34	lr: 0.0005	train loss: 0.0488
35
epoch: 35	lr: 0.0005	train loss: 0.0408
36
epoch: 36	lr: 0.0005	train loss: 0.0413
37
epoch: 37	lr: 0.0005	train loss: 0.0367
38
epoch: 38	lr: 0.0005	train loss: 0.0387
39
epoch: 39	lr: 0.0005	train loss: 0.0360
40
epoch: 40	lr: 0.0005	train loss: 0.0357
41
epoch: 41	lr: 0.0005	train loss: 0.0341
42
epoch: 42	lr: 0.0005	train loss: 0.0401
43
epoch: 43	lr: 0.0005	train loss: 0.0351
44
epoch: 44	lr: 0.0005	train loss: 0.0416
45
epoch: 45	lr: 0.0005	train loss: 0.0406
46
epoch: 46	lr: 0.0005	train loss: 0.0343
47
epoch: 47	lr: 0.0005	train loss: 0.0320
48
epoch: 48	lr: 0.0005	train loss: 0.0296
49
epoch: 49	lr: 0.0005	train loss: 0.0311
Done!
